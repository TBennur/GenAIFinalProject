{"cells":[{"cell_type":"markdown","metadata":{"id":"9MCOFt6mlNZJ"},"source":["# Installations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oqnzt-A3mifm"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1oVpN3c2N7r"},"outputs":[],"source":["%cd /content/drive/MyDrive/GenAIFinalProject/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKRFBeIHgNA3"},"outputs":[],"source":["!pip install pytorch_lightning\n","!pip install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k5whod51ghGv"},"outputs":[],"source":["!sudo apt-get install llvm-9-dev\n","!DS_BUILD_SPARSE_ATTN=1 pip install deepspeed"]},{"cell_type":"markdown","metadata":{"id":"oHWFyyFClQ5X"},"source":["# Dataset"]},{"cell_type":"markdown","source":["Dataset description"],"metadata":{"id":"HwmFOt3Ul1Fp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RsoptKsvlXvo"},"outputs":[],"source":["import torch.utils.data as data\n","import glob\n","import os.path as osp\n","import torch\n","import pytorch_lightning as pl\n","import os\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","\n","root = \"./Data/\"\n","\n","# Custom dataset class that collates saved video sequences for processing\n","class ClipDataset(data.Dataset):\n","\n","    def __init__(self, root):\n","\n","        super().__init__()\n","        self.items = []\n","        files = glob.glob(osp.join(root, '**', f'*.pt'), recursive=True)\n","\n","        print(f\"Loading {len(files)} stores\")\n","        for file in files:\n","          vals = torch.load(file).to(torch.float)\n","          clips = torch.unbind(vals)\n","          self.items += clips\n","        self.sequence_length = self.items[0].shape[1]\n","        self.resolution = self.items[0].shape[2]\n","\n","    def __len__(self):\n","        return len(self.items)\n","\n","    def __getitem__(self, idx):\n","        return self.items[idx]\n","\n","# Adapater for torch lightning\n","class VideoData(pl.LightningDataModule):\n","\n","    def __init__(self, root):\n","        super().__init__()\n","        self.train_root = osp.join(root, \"Train\")\n","        self.test_root = osp.join(root, \"Test\")\n","\n","    def _dataloader(self, files):\n","        dataset = ClipDataset(files)\n","        dataloader = data.DataLoader(\n","            dataset,\n","            batch_size=4,\n","            num_workers=16,\n","            pin_memory=True,\n","            shuffle=True\n","        )\n","        return dataloader\n","\n","    def train_dataloader(self):\n","        return self._dataloader(self.train_root)\n","\n","    def val_dataloader(self):\n","        return self._dataloader(self.test_root)\n","\n","    def test_dataloader(self):\n","        return self.val_dataloader()\n","\n","all_data = VideoData(root)\n","train_dl = all_data.train_dataloader()\n","test_dl = all_data.test_dataloader()"]},{"cell_type":"markdown","metadata":{"id":"fVAx7skpxvpT"},"source":["# VQ-VAE Code"]},{"cell_type":"markdown","source":["The following code is borrowed from the VideoGPT Code: https://github.com/wilson1yan/VideoGPT.git"],"metadata":{"id":"dbYyAkFcnFEy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pThnnu6e-gNu"},"outputs":[],"source":["# Shifts src_tf dim to dest dim\n","# i.e. shift_dim(x, 1, -1) would be (b, c, t, h, w) -> (b, t, h, w, c)\n","def shift_dim(x, src_dim=-1, dest_dim=-1, make_contiguous=True):\n","    n_dims = len(x.shape)\n","    if src_dim < 0:\n","        src_dim = n_dims + src_dim\n","    if dest_dim < 0:\n","        dest_dim = n_dims + dest_dim\n","\n","    assert 0 <= src_dim < n_dims and 0 <= dest_dim < n_dims\n","\n","    dims = list(range(n_dims))\n","    del dims[src_dim]\n","\n","    permutation = []\n","    ctr = 0\n","    for i in range(n_dims):\n","        if i == dest_dim:\n","            permutation.append(src_dim)\n","        else:\n","            permutation.append(dims[ctr])\n","            ctr += 1\n","    x = x.permute(permutation)\n","    if make_contiguous:\n","        x = x.contiguous()\n","    return x\n","\n","# reshapes tensor start from dim i (inclusive)\n","# to dim j (exclusive) to the desired shape\n","# e.g. if x.shape = (b, thw, c) then\n","# view_range(x, 1, 2, (t, h, w)) returns\n","# x of shape (b, t, h, w, c)\n","def view_range(x, i, j, shape):\n","    shape = tuple(shape)\n","\n","    n_dims = len(x.shape)\n","    if i < 0:\n","        i = n_dims + i\n","\n","    if j is None:\n","        j = n_dims\n","    elif j < 0:\n","        j = n_dims + j\n","\n","    assert 0 <= i < j <= n_dims\n","\n","    x_shape = x.shape\n","    target_shape = x_shape[:i] + shape + x_shape[j:]\n","    return x.view(target_shape)\n","\n","\n","def tensor_slice(x, begin, size):\n","    assert all([b >= 0 for b in begin])\n","    size = [l - b if s == -1 else s\n","            for s, b, l in zip(size, begin, x.shape)]\n","    assert all([s >= 0 for s in size])\n","\n","    slices = [slice(b, b + s) for b, s in zip(begin, size)]\n","    return x[slices]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v-Tz6m569Iiu"},"outputs":[],"source":["import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.checkpoint import checkpoint\n","\n","\n","class AttentionStack(nn.Module):\n","    def __init__(\n","        self, shape, embd_dim, n_head, n_layer, dropout,\n","        attn_type, attn_dropout, class_cond_dim, frame_cond_shape,\n","    ):\n","        super().__init__()\n","        self.shape = shape\n","        self.embd_dim = embd_dim\n","        self.use_frame_cond = frame_cond_shape is not None\n","\n","        self.right_shift = RightShift(embd_dim)\n","        self.pos_embd = AddBroadcastPosEmbed(\n","            shape=shape, embd_dim=embd_dim\n","        )\n","\n","        self.attn_nets = nn.ModuleList(\n","            [\n","                AttentionBlock(\n","                    shape=shape,\n","                    embd_dim=embd_dim,\n","                    n_head=n_head,\n","                    n_layer=n_layer,\n","                    dropout=dropout,\n","                    attn_type=attn_type,\n","                    attn_dropout=attn_dropout,\n","                    class_cond_dim=class_cond_dim,\n","                    frame_cond_shape=frame_cond_shape\n","                )\n","                for i in range(n_layer)\n","            ]\n","        )\n","\n","    def forward(self, x, cond, decode_step, decode_idx):\n","        \"\"\"\n","        Args\n","        ------\n","            x: (b, d1, d2, ..., dn, embd_dim)\n","            cond: a dictionary of conditioning tensors\n","\n","            (below is used only when sampling for fast decoding)\n","            decode: the enumerated rasterscan order of the current idx being sampled\n","            decode_step: a tuple representing the current idx being sampled\n","        \"\"\"\n","        x = self.right_shift(x, decode_step)\n","        x = self.pos_embd(x, decode_step, decode_idx)\n","        for net in self.attn_nets:\n","            x = net(x, cond, decode_step, decode_idx)\n","\n","        return x\n","\n","\n","class AttentionBlock(nn.Module):\n","    def __init__(self, shape, embd_dim, n_head, n_layer, dropout,\n","                 attn_type, attn_dropout, class_cond_dim, frame_cond_shape):\n","        super().__init__()\n","        self.use_frame_cond = frame_cond_shape is not None\n","\n","        self.pre_attn_norm = LayerNorm(embd_dim, class_cond_dim)\n","        self.post_attn_dp = nn.Dropout(dropout)\n","        self.attn = MultiHeadAttention(shape, embd_dim, embd_dim, n_head,\n","                                       n_layer, causal=True, attn_type=attn_type,\n","                                       attn_kwargs=dict(attn_dropout=attn_dropout))\n","\n","        if frame_cond_shape is not None:\n","            enc_len = np.prod(frame_cond_shape[:-1])\n","            self.pre_enc_norm = LayerNorm(embd_dim, class_cond_dim)\n","            self.post_enc_dp = nn.Dropout(dropout)\n","            self.enc_attn = MultiHeadAttention(shape, embd_dim, frame_cond_shape[-1],\n","                                               n_head, n_layer, attn_type='full',\n","                                               attn_kwargs=dict(attn_dropout=0.), causal=False)\n","\n","        self.pre_fc_norm = LayerNorm(embd_dim, class_cond_dim)\n","        self.post_fc_dp = nn.Dropout(dropout)\n","        self.fc_block = nn.Sequential(\n","            nn.Linear(in_features=embd_dim, out_features=embd_dim * 4),\n","            GeLU2(),\n","            nn.Linear(in_features=embd_dim * 4, out_features=embd_dim),\n","        )\n","\n","    def forward(self, x, cond, decode_step, decode_idx):\n","        h = self.pre_attn_norm(x, cond)\n","        if self.training:\n","            h = checkpoint(self.attn, h, h, h, decode_step, decode_idx)\n","        else:\n","            h = self.attn(h, h, h, decode_step, decode_idx)\n","        h = self.post_attn_dp(h)\n","        x = x + h\n","\n","        if self.use_frame_cond:\n","            h = self.pre_enc_norm(x, cond)\n","            if self.training:\n","                h = checkpoint(self.enc_attn, h, cond['frame_cond'], cond['frame_cond'],\n","                               decode_step, decode_idx)\n","            else:\n","                h = self.enc_attn(h, cond['frame_cond'], cond['frame_cond'],\n","                                  decode_step, decode_idx)\n","            h = self.post_enc_dp(h)\n","            x = x + h\n","\n","        h = self.pre_fc_norm(x, cond)\n","        if self.training:\n","            h = checkpoint(self.fc_block, h)\n","        else:\n","            h = self.fc_block(h)\n","        h = self.post_fc_dp(h)\n","        x = x + h\n","\n","        return x\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, shape, dim_q, dim_kv, n_head, n_layer,\n","                 causal, attn_type, attn_kwargs):\n","        super().__init__()\n","        self.causal = causal\n","        self.shape = shape\n","\n","        self.d_k = dim_q // n_head\n","        self.d_v = dim_kv // n_head\n","        self.n_head = n_head\n","\n","        self.w_qs = nn.Linear(dim_q, n_head * self.d_k, bias=False) # q\n","        self.w_qs.weight.data.normal_(std=1.0 / np.sqrt(dim_q))\n","\n","        self.w_ks = nn.Linear(dim_kv, n_head * self.d_k, bias=False) # k\n","        self.w_ks.weight.data.normal_(std=1.0 / np.sqrt(dim_kv))\n","\n","        self.w_vs = nn.Linear(dim_kv, n_head * self.d_v, bias=False) # v\n","        self.w_vs.weight.data.normal_(std=1.0 / np.sqrt(dim_kv))\n","\n","        self.fc = nn.Linear(n_head * self.d_v, dim_q, bias=True) # c\n","        self.fc.weight.data.normal_(std=1.0 / np.sqrt(dim_q * n_layer))\n","\n","        if attn_type == 'full':\n","            self.attn = FullAttention(shape, causal, **attn_kwargs)\n","        elif attn_type == 'axial':\n","            assert not causal, 'causal axial attention is not supported'\n","            self.attn = AxialAttention(len(shape), **attn_kwargs)\n","        elif attn_type == 'sparse':\n","            self.attn = SparseAttention(shape, n_head, causal, **attn_kwargs)\n","\n","        self.cache = None\n","\n","    def forward(self, q, k, v, decode_step=None, decode_idx=None):\n","        \"\"\" Compute multi-head attention\n","        Args\n","            q, k, v: a [b, d1, ..., dn, c] tensor or\n","                     a [b, 1, ..., 1, c] tensor if decode_step is not None\n","\n","        Returns\n","            The output after performing attention\n","        \"\"\"\n","\n","        # compute k, q, v\n","        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n","        q = view_range(self.w_qs(q), -1, None, (n_head, d_k))\n","        k = view_range(self.w_ks(k), -1, None, (n_head, d_k))\n","        v = view_range(self.w_vs(v), -1, None, (n_head, d_v))\n","\n","        # b x n_head x seq_len x d\n","        # (b, *d_shape, n_head, d) -> (b, n_head, *d_shape, d)\n","        q = shift_dim(q, -2, 1)\n","        k = shift_dim(k, -2, 1)\n","        v = shift_dim(v, -2, 1)\n","\n","        # fast decoding\n","        if decode_step is not None:\n","            if decode_step == 0:\n","                if self.causal:\n","                    k_shape = (q.shape[0], n_head, *self.shape, self.d_k)\n","                    v_shape = (q.shape[0], n_head, *self.shape, self.d_v)\n","                    self.cache = dict(k=torch.zeros(k_shape, dtype=k.dtype, device=q.device),\n","                                    v=torch.zeros(v_shape, dtype=v.dtype, device=q.device))\n","                else:\n","                    # cache only once in the non-causal case\n","                    self.cache = dict(k=k.clone(), v=v.clone())\n","            if self.causal:\n","                idx = (slice(None, None), slice(None, None), *[slice(i, i+ 1) for i in decode_idx])\n","                self.cache['k'][idx] = k\n","                self.cache['v'][idx] = v\n","            k, v = self.cache['k'], self.cache['v']\n","\n","        a = self.attn(q, k, v, decode_step, decode_idx)\n","\n","        # (b, *d_shape, n_head, d) -> (b, *d_shape, n_head * d)\n","        a = shift_dim(a, 1, -2).flatten(start_dim=-2)\n","        a = self.fc(a) # (b x seq_len x embd_dim)\n","\n","        return a\n","\n","############## Attention #######################\n","class FullAttention(nn.Module):\n","    def __init__(self, shape, causal, attn_dropout):\n","        super().__init__()\n","        self.causal = causal\n","        self.attn_dropout = attn_dropout\n","\n","        seq_len = np.prod(shape)\n","        if self.causal:\n","            self.register_buffer('mask', torch.tril(torch.ones(seq_len, seq_len)))\n","\n","    def forward(self, q, k, v, decode_step, decode_idx):\n","        mask = self.mask if self.causal else None\n","        if decode_step is not None and mask is not None:\n","            mask = mask[[decode_step]]\n","\n","        old_shape = q.shape[2:-1]\n","        q = q.flatten(start_dim=2, end_dim=-2)\n","        k = k.flatten(start_dim=2, end_dim=-2)\n","        v = v.flatten(start_dim=2, end_dim=-2)\n","\n","        out = scaled_dot_product_attention(q, k, v, mask=mask,\n","                                           attn_dropout=self.attn_dropout,\n","                                           training=self.training)\n","\n","        return view_range(out, 2, 3, old_shape)\n","\n","class AxialAttention(nn.Module):\n","    def __init__(self, n_dim, axial_dim):\n","        super().__init__()\n","        if axial_dim < 0:\n","            axial_dim = 2 + n_dim + 1 + axial_dim\n","        else:\n","            axial_dim += 2 # account for batch, head, dim\n","        self.axial_dim = axial_dim\n","\n","    def forward(self, q, k, v, decode_step, decode_idx):\n","        q = shift_dim(q, self.axial_dim, -2).flatten(end_dim=-3)\n","        k = shift_dim(k, self.axial_dim, -2).flatten(end_dim=-3)\n","        v = shift_dim(v, self.axial_dim, -2)\n","        old_shape = list(v.shape)\n","        v = v.flatten(end_dim=-3)\n","\n","        out = scaled_dot_product_attention(q, k, v, training=self.training)\n","        out = out.view(*old_shape)\n","        out = shift_dim(out, -2, self.axial_dim)\n","        return out\n","\n","\n","class SparseAttention(nn.Module):\n","    ops = dict()\n","    attn_mask = dict()\n","    block_layout = dict()\n","\n","    def __init__(self, shape, n_head, causal, num_local_blocks=4, block=32,\n","                 attn_dropout=0.): # does not use attn_dropout\n","        super().__init__()\n","        self.causal = causal\n","        self.shape = shape\n","\n","        self.sparsity_config = StridedSparsityConfig(shape=shape, n_head=n_head,\n","                                                     causal=causal, block=block,\n","                                                     num_local_blocks=num_local_blocks)\n","\n","        if self.shape not in SparseAttention.block_layout:\n","            SparseAttention.block_layout[self.shape] = self.sparsity_config.make_layout()\n","        if causal and self.shape not in SparseAttention.attn_mask:\n","            SparseAttention.attn_mask[self.shape] = self.sparsity_config.make_sparse_attn_mask()\n","\n","    def get_ops(self):\n","        try:\n","            from deepspeed.ops.sparse_attention import MatMul, Softmax\n","        except:\n","            raise Exception('Error importing deepspeed. Please install using `DS_BUILD_SPARSE_ATTN=1 pip install deepspeed`')\n","        if self.shape not in SparseAttention.ops:\n","            sparsity_layout = self.sparsity_config.make_layout()\n","            sparse_dot_sdd_nt = MatMul(sparsity_layout,\n","                                       self.sparsity_config.block,\n","                                       'sdd',\n","                                       trans_a=False,\n","                                       trans_b=True)\n","\n","            sparse_dot_dsd_nn = MatMul(sparsity_layout,\n","                                       self.sparsity_config.block,\n","                                       'dsd',\n","                                       trans_a=False,\n","                                       trans_b=False)\n","\n","            sparse_softmax = Softmax(sparsity_layout, self.sparsity_config.block)\n","\n","            SparseAttention.ops[self.shape] = (sparse_dot_sdd_nt,\n","                                               sparse_dot_dsd_nn,\n","                                               sparse_softmax)\n","        return SparseAttention.ops[self.shape]\n","\n","    def forward(self, q, k, v, decode_step, decode_idx):\n","        if self.training and self.shape not in SparseAttention.ops:\n","            self.get_ops()\n","\n","        SparseAttention.block_layout[self.shape] = SparseAttention.block_layout[self.shape].to(q)\n","        if self.causal:\n","            SparseAttention.attn_mask[self.shape] = SparseAttention.attn_mask[self.shape].to(q).type_as(q)\n","        attn_mask = SparseAttention.attn_mask[self.shape] if self.causal else None\n","\n","        old_shape = q.shape[2:-1]\n","        q = q.flatten(start_dim=2, end_dim=-2)\n","        k = k.flatten(start_dim=2, end_dim=-2)\n","        v = v.flatten(start_dim=2, end_dim=-2)\n","\n","        if decode_step is not None:\n","            mask = self.sparsity_config.get_non_block_layout_row(SparseAttention.block_layout[self.shape], decode_step)\n","            out = scaled_dot_product_attention(q, k, v, mask=mask, training=self.training)\n","        else:\n","            if q.shape != k.shape or k.shape != v.shape:\n","                raise Exception('SparseAttention only support self-attention')\n","            sparse_dot_sdd_nt, sparse_dot_dsd_nn, sparse_softmax = self.get_ops()\n","            scaling = float(q.shape[-1]) ** -0.5\n","\n","            attn_output_weights = sparse_dot_sdd_nt(q, k)\n","            if attn_mask is not None:\n","                attn_output_weights = attn_output_weights.masked_fill(attn_mask == 0,\n","                                                                      float('-inf'))\n","            attn_output_weights = sparse_softmax(\n","                attn_output_weights,\n","                scale=scaling\n","            )\n","\n","            out = sparse_dot_dsd_nn(attn_output_weights, v)\n","\n","        return view_range(out, 2, 3, old_shape)\n","\n","\n","class StridedSparsityConfig(object):\n","    \"\"\"\n","    Strided Sparse configuration specified in https://arxiv.org/abs/1904.10509 that\n","    generalizes to arbitrary dimensions\n","    \"\"\"\n","    def __init__(self, shape, n_head, causal, block, num_local_blocks):\n","        self.n_head = n_head\n","        self.shape = shape\n","        self.causal = causal\n","        self.block = block\n","        self.num_local_blocks = num_local_blocks\n","\n","        assert self.num_local_blocks >= 1, 'Must have at least 1 local block'\n","        assert self.seq_len % self.block == 0, 'seq len must be divisible by block size'\n","\n","        self._block_shape = self._compute_block_shape()\n","        self._block_shape_cum = self._block_shape_cum_sizes()\n","\n","    @property\n","    def seq_len(self):\n","        return np.prod(self.shape)\n","\n","    @property\n","    def num_blocks(self):\n","        return self.seq_len // self.block\n","\n","    def set_local_layout(self, layout):\n","        num_blocks = self.num_blocks\n","        for row in range(0, num_blocks):\n","            end = min(row + self.num_local_blocks, num_blocks)\n","            for col in range(\n","                    max(0, row - self.num_local_blocks),\n","                    (row + 1 if self.causal else end)):\n","                layout[:, row, col] = 1\n","        return layout\n","\n","    def set_global_layout(self, layout):\n","        num_blocks = self.num_blocks\n","        n_dim = len(self._block_shape)\n","        for row in range(num_blocks):\n","            assert self._to_flattened_idx(self._to_unflattened_idx(row)) == row\n","            cur_idx = self._to_unflattened_idx(row)\n","            # no strided attention over last dim\n","            for d in range(n_dim - 1):\n","                end = self._block_shape[d]\n","                for i in range(0, (cur_idx[d] + 1 if self.causal else end)):\n","                    new_idx = list(cur_idx)\n","                    new_idx[d] = i\n","                    new_idx = tuple(new_idx)\n","\n","                    col = self._to_flattened_idx(new_idx)\n","                    layout[:, row, col] = 1\n","\n","        return layout\n","\n","    def make_layout(self):\n","        layout = torch.zeros((self.n_head, self.num_blocks, self.num_blocks), dtype=torch.int64)\n","        layout = self.set_local_layout(layout)\n","        layout = self.set_global_layout(layout)\n","        return layout\n","\n","    def make_sparse_attn_mask(self):\n","        block_layout = self.make_layout()\n","        assert block_layout.shape[1] == block_layout.shape[2] == self.num_blocks\n","\n","        num_dense_blocks = block_layout.sum().item()\n","        attn_mask = torch.ones(num_dense_blocks, self.block, self.block)\n","        counter = 0\n","        for h in range(self.n_head):\n","            for i in range(self.num_blocks):\n","                for j in range(self.num_blocks):\n","                    elem = block_layout[h, i, j].item()\n","                    if elem == 1:\n","                        assert i >= j\n","                        if i == j: # need to mask within block on diagonals\n","                            attn_mask[counter] = torch.tril(attn_mask[counter])\n","                        counter += 1\n","        assert counter == num_dense_blocks\n","\n","        return attn_mask.unsqueeze(0)\n","\n","    def get_non_block_layout_row(self, block_layout, row):\n","        block_row = row // self.block\n","        block_row = block_layout[:, [block_row]] # n_head x 1 x n_blocks\n","        block_row = block_row.repeat_interleave(self.block, dim=-1)\n","        block_row[:, :, row + 1:] = 0.\n","        return block_row\n","\n","    ############# Helper functions ##########################\n","\n","    def _compute_block_shape(self):\n","        n_dim = len(self.shape)\n","        cum_prod = 1\n","        for i in range(n_dim - 1, -1, -1):\n","            cum_prod *= self.shape[i]\n","            if cum_prod > self.block:\n","                break\n","        assert cum_prod % self.block == 0\n","        new_shape = (*self.shape[:i], cum_prod // self.block)\n","\n","        assert np.prod(new_shape) == np.prod(self.shape) // self.block\n","\n","        return new_shape\n","\n","    def _block_shape_cum_sizes(self):\n","        bs = np.flip(np.array(self._block_shape))\n","        return tuple(np.flip(np.cumprod(bs)[:-1])) + (1,)\n","\n","    def _to_flattened_idx(self, idx):\n","        assert len(idx) == len(self._block_shape), f\"{len(idx)} != {len(self._block_shape)}\"\n","        flat_idx = 0\n","        for i in range(len(self._block_shape)):\n","            flat_idx += idx[i] * self._block_shape_cum[i]\n","        return flat_idx\n","\n","    def _to_unflattened_idx(self, flat_idx):\n","        assert flat_idx < np.prod(self._block_shape)\n","        idx = []\n","        for i in range(len(self._block_shape)):\n","            idx.append(flat_idx // self._block_shape_cum[i])\n","            flat_idx %= self._block_shape_cum[i]\n","        return tuple(idx)\n","\n","\n","################ Spatiotemporal broadcasted positional embeddings ###############\n","class AddBroadcastPosEmbed(nn.Module):\n","    def __init__(self, shape, embd_dim, dim=-1):\n","        super().__init__()\n","        assert dim in [-1, 1] # only first or last dim supported\n","        self.shape = shape\n","        self.n_dim = n_dim = len(shape)\n","        self.embd_dim = embd_dim\n","        self.dim = dim\n","\n","        assert embd_dim % n_dim == 0, f\"{embd_dim} % {n_dim} != 0\"\n","        self.emb = nn.ParameterDict({\n","             f'd_{i}': nn.Parameter(torch.randn(shape[i], embd_dim // n_dim) * 0.01\n","                                    if dim == -1 else\n","                                    torch.randn(embd_dim // n_dim, shape[i]) * 0.01)\n","             for i in range(n_dim)\n","        })\n","\n","    def forward(self, x, decode_step=None, decode_idx=None):\n","        embs = []\n","        for i in range(self.n_dim):\n","            e = self.emb[f'd_{i}']\n","            if self.dim == -1:\n","                # (1, 1, ..., 1, self.shape[i], 1, ..., -1)\n","                e = e.view(1, *((1,) * i), self.shape[i], *((1,) * (self.n_dim - i - 1)), -1)\n","                e = e.expand(1, *self.shape, -1)\n","            else:\n","                e = e.view(1, -1, *((1,) * i), self.shape[i], *((1,) * (self.n_dim - i - 1)))\n","                e = e.expand(1, -1, *self.shape)\n","            embs.append(e)\n","\n","        embs = torch.cat(embs, dim=self.dim)\n","        if decode_step is not None:\n","            embs = tensor_slice(embs, [0, *decode_idx, 0],\n","                                [x.shape[0], *(1,) * self.n_dim, x.shape[-1]])\n","\n","        return x + embs\n","\n","################# Helper Functions ###################################\n","def scaled_dot_product_attention(q, k, v, mask=None, attn_dropout=0., training=True):\n","    # Performs scaled dot-product attention over the second to last dimension dn\n","\n","    # (b, n_head, d1, ..., dn, d)\n","    attn = torch.matmul(q, k.transpose(-1, -2))\n","    attn = attn / np.sqrt(q.shape[-1])\n","    if mask is not None:\n","        attn = attn.masked_fill(mask == 0, float('-inf'))\n","    attn_float = F.softmax(attn, dim=-1)\n","    attn = attn_float.type_as(attn) # b x n_head x d1 x ... x dn x d\n","    attn = F.dropout(attn, p=attn_dropout, training=training)\n","\n","    a = torch.matmul(attn, v) # b x n_head x d1 x ... x dn x d\n","\n","    return a\n","\n","\n","class RightShift(nn.Module):\n","    def __init__(self, embd_dim):\n","        super().__init__()\n","        self.embd_dim = embd_dim\n","        self.sos = nn.Parameter(torch.FloatTensor(embd_dim).normal_(std=0.02), requires_grad=True)\n","\n","    def forward(self, x, decode_step):\n","        if decode_step is not None and decode_step > 0:\n","            return x\n","\n","        x_shape = list(x.shape)\n","        x = x.flatten(start_dim=1, end_dim=-2) # (b, seq_len, embd_dim)\n","        sos = torch.ones(x_shape[0], 1, self.embd_dim, dtype=torch.float32).to(self.sos) * self.sos\n","        sos = sos.type_as(x)\n","        x = torch.cat([sos, x[:, :-1, :]], axis=1)\n","        x = x.view(*x_shape)\n","\n","        return x\n","\n","\n","class GeLU2(nn.Module):\n","    def forward(self, x):\n","        return (1.702 * x).sigmoid() * x\n","\n","\n","class LayerNorm(nn.Module):\n","    def __init__(self, embd_dim, class_cond_dim):\n","        super().__init__()\n","        self.conditional = class_cond_dim is not None\n","\n","        if self.conditional:\n","            self.w = nn.Linear(class_cond_dim, embd_dim, bias=False)\n","            nn.init.constant_(self.w.weight.data, 1. / np.sqrt(class_cond_dim))\n","            self.wb = nn.Linear(class_cond_dim, embd_dim, bias=False)\n","        else:\n","            self.g = nn.Parameter(torch.ones(embd_dim, dtype=torch.float32), requires_grad=True)\n","            self.b = nn.Parameter(torch.zeros(embd_dim, dtype=torch.float32), requires_grad=True)\n","\n","    def forward(self, x, cond):\n","        if self.conditional:  # (b, cond_dim)\n","            g = 1 + self.w(cond['class_cond']).view(x.shape[0], *(1,)*(len(x.shape)-2), x.shape[-1]) # (b, ..., embd_dim)\n","            b = self.wb(cond['class_cond']).view(x.shape[0], *(1,)*(len(x.shape)-2), x.shape[-1])\n","        else:\n","            g = self.g  # (embd_dim,)\n","            b = self.b\n","\n","        x_float = x.float()\n","\n","        mu = x_float.mean(dim=-1, keepdims=True)\n","        s = (x_float - mu).square().mean(dim=-1, keepdims=True)\n","        x_float = (x_float - mu) * (1e-5 + s.rsqrt())  # (b, ..., embd_dim)\n","        x_float = x_float * g + b\n","\n","        x = x_float.type_as(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQuR76qkxztJ"},"outputs":[],"source":["import math\n","import argparse\n","import numpy as np\n","\n","import pytorch_lightning as pl\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.distributed as dist\n","import wandb\n","\n","N_RES = 4\n","N_HIDDEN = 120\n","N_CHANNELS = 128\n","\n","class VQVAE(pl.LightningModule):\n","    def __init__(self):\n","        super().__init__()\n","        self.embedding_dim = N_CHANNELS\n","        self.n_codes = 2048\n","\n","        self.encoder = Encoder(N_HIDDEN, N_RES, (1, 4, 4))\n","        self.decoder = Decoder(N_HIDDEN, N_RES, (1, 4, 4))\n","\n","        self.pre_vq_conv = SamePadConv3d(N_HIDDEN, N_CHANNELS, 1)\n","        self.post_vq_conv = SamePadConv3d(N_CHANNELS, N_HIDDEN, 1)\n","\n","        self.codebook = Codebook(2048, N_CHANNELS)\n","        self.save_hyperparameters()\n","\n","    @property\n","    def latent_shape(self):\n","        input_shape = (self.args.sequence_length, self.args.resolution,\n","                       self.args.resolution)\n","        return tuple([s // d for s, d in zip(input_shape,\n","                                             self.args.downsample)])\n","\n","    def encode(self, x, include_embeddings=False):\n","        h = self.pre_vq_conv(self.encoder(x))\n","        vq_output = self.codebook(h)\n","        if include_embeddings:\n","            return vq_output['encodings'], vq_output['embeddings']\n","        else:\n","            return vq_output['encodings']\n","\n","    def decode(self, encodings):\n","        h = F.embedding(encodings, self.codebook.embeddings)\n","        h = self.post_vq_conv(shift_dim(h, -1, 1))\n","        return self.decoder(h)\n","\n","    def forward(self, x):\n","        z = self.pre_vq_conv(self.encoder(x))\n","        vq_output = self.codebook(z)\n","        x_recon = self.decoder(self.post_vq_conv(vq_output['embeddings']))\n","        recon_loss = F.mse_loss(x_recon, x) / 0.06\n","\n","        return recon_loss, x_recon, vq_output\n","\n","    def training_step(self, batch, batch_idx):\n","        x = batch\n","        recon_loss, _, vq_output = self.forward(x)\n","        commitment_loss = vq_output['commitment_loss']\n","        loss = recon_loss + commitment_loss\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x = batch\n","        recon_loss, _, vq_output = self.forward(x)\n","        self.log('val/recon_loss', recon_loss, prog_bar=True)\n","        self.log('val/perplexity', vq_output['perplexity'], prog_bar=True)\n","        self.log('val/commitment_loss', vq_output['commitment_loss'], prog_bar=True)\n","        wandb.log({\"val/recon_loss\": recon_loss, \"val/perplexity\": vq_output['perplexity'], \"val/commitment_loss\": vq_output['commitment_loss']})\n","\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=3e-4, betas=(0.9, 0.999))\n","\n","class AxialBlock(nn.Module):\n","    def __init__(self, n_hiddens, n_head):\n","        super().__init__()\n","        kwargs = dict(shape=(0,) * 3, dim_q=n_hiddens,\n","                      dim_kv=n_hiddens, n_head=n_head,\n","                      n_layer=1, causal=False, attn_type='axial')\n","        self.attn_w = MultiHeadAttention(attn_kwargs=dict(axial_dim=-2),\n","                                         **kwargs)\n","        self.attn_h = MultiHeadAttention(attn_kwargs=dict(axial_dim=-3),\n","                                         **kwargs)\n","        self.attn_t = MultiHeadAttention(attn_kwargs=dict(axial_dim=-4),\n","                                         **kwargs)\n","\n","    def forward(self, x):\n","        x = shift_dim(x, 1, -1)\n","        x = self.attn_w(x, x, x) + self.attn_h(x, x, x) + self.attn_t(x, x, x)\n","        x = shift_dim(x, -1, 1)\n","        return x\n","\n","\n","class AttentionResidualBlock(nn.Module):\n","    def __init__(self, n_hiddens):\n","        super().__init__()\n","        self.block = nn.Sequential(\n","            nn.BatchNorm3d(n_hiddens),\n","            nn.ReLU(),\n","            SamePadConv3d(n_hiddens, n_hiddens // 2, 3, bias=False),\n","            nn.BatchNorm3d(n_hiddens // 2),\n","            nn.ReLU(),\n","            SamePadConv3d(n_hiddens // 2, n_hiddens, 1, bias=False),\n","            nn.BatchNorm3d(n_hiddens),\n","            nn.ReLU(),\n","            AxialBlock(n_hiddens, 2)\n","        )\n","\n","    def forward(self, x):\n","        return x + self.block(x)\n","\n","class Codebook(nn.Module):\n","    def __init__(self, n_codes, embedding_dim):\n","        super().__init__()\n","        self.register_buffer('embeddings', torch.randn(n_codes, embedding_dim))\n","        self.register_buffer('N', torch.zeros(n_codes))\n","        self.register_buffer('z_avg', self.embeddings.data.clone())\n","\n","        self.n_codes = n_codes\n","        self.embedding_dim = embedding_dim\n","        self._need_init = True\n","\n","    def _tile(self, x):\n","        d, ew = x.shape\n","        if d < self.n_codes:\n","            n_repeats = (self.n_codes + d - 1) // d\n","            std = 0.01 / np.sqrt(ew)\n","            x = x.repeat(n_repeats, 1)\n","            x = x + torch.randn_like(x) * std\n","        return x\n","\n","    def _init_embeddings(self, z):\n","        # z: [b, c, t, h, w]\n","        self._need_init = False\n","        flat_inputs = shift_dim(z, 1, -1).flatten(end_dim=-2)\n","        y = self._tile(flat_inputs)\n","\n","        d = y.shape[0]\n","        _k_rand = y[torch.randperm(y.shape[0])][:self.n_codes]\n","        if dist.is_initialized():\n","            dist.broadcast(_k_rand, 0)\n","        self.embeddings.data.copy_(_k_rand)\n","        self.z_avg.data.copy_(_k_rand)\n","        self.N.data.copy_(torch.ones(self.n_codes))\n","\n","    def forward(self, z):\n","        # z: [b, c, t, h, w]\n","        if self._need_init and self.training:\n","            self._init_embeddings(z)\n","        flat_inputs = shift_dim(z, 1, -1).flatten(end_dim=-2)\n","        distances = (flat_inputs ** 2).sum(dim=1, keepdim=True) \\\n","                    - 2 * flat_inputs @ self.embeddings.t() \\\n","                    + (self.embeddings.t() ** 2).sum(dim=0, keepdim=True)\n","\n","        encoding_indices = torch.argmin(distances, dim=1)\n","        encode_onehot = F.one_hot(encoding_indices, self.n_codes).type_as(flat_inputs)\n","        encoding_indices = encoding_indices.view(z.shape[0], *z.shape[2:])\n","\n","        embeddings = F.embedding(encoding_indices, self.embeddings)\n","        embeddings = shift_dim(embeddings, -1, 1)\n","\n","        commitment_loss = 0.25 * F.mse_loss(z, embeddings.detach())\n","\n","        # EMA codebook update\n","        if self.training:\n","            n_total = encode_onehot.sum(dim=0)\n","            encode_sum = flat_inputs.t() @ encode_onehot\n","            if dist.is_initialized():\n","                dist.all_reduce(n_total)\n","                dist.all_reduce(encode_sum)\n","\n","            self.N.data.mul_(0.99).add_(n_total, alpha=0.01)\n","            self.z_avg.data.mul_(0.99).add_(encode_sum.t(), alpha=0.01)\n","\n","            n = self.N.sum()\n","            weights = (self.N + 1e-7) / (n + self.n_codes * 1e-7) * n\n","            encode_normalized = self.z_avg / weights.unsqueeze(1)\n","            self.embeddings.data.copy_(encode_normalized)\n","\n","            y = self._tile(flat_inputs)\n","            _k_rand = y[torch.randperm(y.shape[0])][:self.n_codes]\n","            if dist.is_initialized():\n","                dist.broadcast(_k_rand, 0)\n","\n","            usage = (self.N.view(self.n_codes, 1) >= 1).float()\n","            self.embeddings.data.mul_(usage).add_(_k_rand * (1 - usage))\n","\n","        embeddings_st = (embeddings - z).detach() + z\n","\n","        avg_probs = torch.mean(encode_onehot, dim=0)\n","        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n","\n","        return dict(embeddings=embeddings_st, encodings=encoding_indices,\n","                    commitment_loss=commitment_loss, perplexity=perplexity)\n","\n","    def dictionary_lookup(self, encodings):\n","        embeddings = F.embedding(encodings, self.embeddings)\n","        return embeddings\n","\n","class Encoder(nn.Module):\n","    def __init__(self, n_hiddens, n_res_layers, downsample):\n","        super().__init__()\n","        n_times_downsample = np.array([int(math.log2(d)) for d in downsample])\n","        self.convs = nn.ModuleList()\n","        max_ds = n_times_downsample.max()\n","        for i in range(max_ds):\n","            in_channels = 3 if i == 0 else n_hiddens\n","            stride = tuple([2 if d > 0 else 1 for d in n_times_downsample])\n","            conv = SamePadConv3d(in_channels, n_hiddens, 4, stride=stride)\n","            self.convs.append(conv)\n","            n_times_downsample -= 1\n","        self.conv_last = SamePadConv3d(in_channels, n_hiddens, kernel_size=3)\n","\n","        self.res_stack = nn.Sequential(\n","            *[AttentionResidualBlock(n_hiddens)\n","              for _ in range(n_res_layers)],\n","            nn.BatchNorm3d(n_hiddens),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x):\n","        h = x\n","        for conv in self.convs:\n","            h = F.relu(conv(h))\n","        h = self.conv_last(h)\n","        h = self.res_stack(h)\n","        return h\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, n_hiddens, n_res_layers, upsample):\n","        super().__init__()\n","        self.res_stack = nn.Sequential(\n","            *[AttentionResidualBlock(n_hiddens)\n","              for _ in range(n_res_layers)],\n","            nn.BatchNorm3d(n_hiddens),\n","            nn.ReLU()\n","        )\n","\n","        n_times_upsample = np.array([int(math.log2(d)) for d in upsample])\n","        max_us = n_times_upsample.max()\n","        self.convts = nn.ModuleList()\n","        for i in range(max_us):\n","            out_channels = 3 if i == max_us - 1 else n_hiddens\n","            us = tuple([2 if d > 0 else 1 for d in n_times_upsample])\n","            convt = SamePadConvTranspose3d(n_hiddens, out_channels, 4,\n","                                           stride=us)\n","            self.convts.append(convt)\n","            n_times_upsample -= 1\n","\n","    def forward(self, x):\n","        h = self.res_stack(x)\n","        for i, convt in enumerate(self.convts):\n","            h = convt(h)\n","            if i < len(self.convts) - 1:\n","                h = F.relu(h)\n","        return h\n","\n","\n","# Does not support dilation\n","class SamePadConv3d(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1, bias=True):\n","        super().__init__()\n","        if isinstance(kernel_size, int):\n","            kernel_size = (kernel_size,) * 3\n","        if isinstance(stride, int):\n","            stride = (stride,) * 3\n","\n","        # assumes that the input shape is divisible by stride\n","        total_pad = tuple([k - s for k, s in zip(kernel_size, stride)])\n","        pad_input = []\n","        for p in total_pad[::-1]: # reverse since F.pad starts from last dim\n","            pad_input.append((p // 2 + p % 2, p // 2))\n","        pad_input = sum(pad_input, tuple())\n","        self.pad_input = pad_input\n","\n","        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size,\n","                              stride=stride, padding=0, bias=bias)\n","\n","    def forward(self, x):\n","        return self.conv(F.pad(x, self.pad_input))\n","\n","\n","class SamePadConvTranspose3d(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1, bias=True):\n","        super().__init__()\n","        if isinstance(kernel_size, int):\n","            kernel_size = (kernel_size,) * 3\n","        if isinstance(stride, int):\n","            stride = (stride,) * 3\n","\n","        total_pad = tuple([k - s for k, s in zip(kernel_size, stride)])\n","        pad_input = []\n","        for p in total_pad[::-1]: # reverse since F.pad starts from last dim\n","            pad_input.append((p // 2 + p % 2, p // 2))\n","        pad_input = sum(pad_input, tuple())\n","        self.pad_input = pad_input\n","\n","        self.convt = nn.ConvTranspose3d(in_channels, out_channels, kernel_size,\n","                                        stride=stride, bias=bias,\n","                                        padding=tuple([k - 1 for k in kernel_size]))\n","\n","    def forward(self, x):\n","        return self.convt(F.pad(x, self.pad_input))"]},{"cell_type":"markdown","metadata":{"id":"skfjsrZVlICM"},"source":["# Training the VQ-VAE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORF8ww6DkQ-L"},"outputs":[],"source":["# Borrowed and modified from the VideoGPT github: https://github.com/wilson1yan/VideoGPT.git\n","\n","import argparse\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from pytorch_lightning.loggers import WandbLogger\n","import wandb\n","import gc\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","wandb.login(key=\"f9e8ebaee8545d7ff6e3b1afa4f192d01079b34a\")\n","\n","def main():\n","    pl.seed_everything(1234)\n","\n","    parser = argparse.ArgumentParser()\n","\n","    data = VideoData(root)\n","    model = VQVAE()\n","\n","    callbacks = []\n","    callbacks.append(ModelCheckpoint(monitor='val/recon_loss', mode='min'))\n","\n","    kwargs = dict()\n","    wandb_logger = WandbLogger(project=\"VideoGPT VQ-VAE\")\n","\n","    trainer = pl.Trainer(accelerator=\"gpu\", max_epochs=200, logger = wandb_logger)\n","\n","    trainer.fit(model, data)\n","\n","\n","if __name__ == '__main__':\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}