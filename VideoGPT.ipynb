{"cells":[{"cell_type":"markdown","source":["# Setu"],"metadata":{"id":"OtT1rW4rg2M0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rs0qZRGTMQ5N"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TvrJZaYWT4Hd"},"outputs":[],"source":["%cd /content/drive/MyDrive/GenAIFinalProject/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"57WSvz5eo7y6"},"outputs":[],"source":["root = \"/content/drive/MyDrive/GenAIFinalProject/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9g5yLpQNDao"},"outputs":[],"source":["!pip install -r /content/drive/MyDrive/GenAIFinalProject/requirements.txt\n","!pip install importnb\n","!pip install wandb\n","!pip install import-ipynb\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mV5DNxtXDiny"},"outputs":[],"source":["!pip install ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJbw06a4MKwo"},"outputs":[],"source":["import os\n","import itertools\n","import numpy as np\n","import argparse\n","import gdown\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim.lr_scheduler as lr_scheduler\n","import pytorch_lightning as pl\n","import os.path as osp\n","import wandb\n","import gc\n","\n","from tqdm import tqdm\n","from torchmetrics.image import PeakSignalNoiseRatio\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from pytorch_lightning.loggers import WandbLogger\n","\n","\n","from resnet import resnet34\n","from attention import AttentionStack, LayerNorm, AddBroadcastPosEmbed\n","from utils import shift_dim\n","from importnb import imports\n","import import_ipynb\n","\n","from videotransformerdataset import VideoDataset\n","\n","from vq_vae import VQVAE\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fo9p0XKJlmSO"},"outputs":[],"source":["import torch.utils.data as data\n","import glob\n","import os.path as osp\n","import torch\n","import pytorch_lightning as pl\n","import os\n","from torch.utils.data import SubsetRandomSampler\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","\n","# Custom dataset class\n","class ClipDataset(data.Dataset):\n","\n","    def __init__(self, root):\n","\n","        super().__init__()\n","        self.items = []\n","        files = glob.glob(osp.join(root, '**', f'*.pt'), recursive=True)\n","\n","        print(f\"Loading {len(files)} stores\")\n","        for file in files:\n","          vals = torch.load(file).to(torch.float)\n","          clips = torch.unbind(vals)\n","          self.items += clips\n","        self.sequence_length = self.items[0].shape[1]\n","        self.resolution = self.items[0].shape[2]\n","\n","    def __len__(self):\n","        return len(self.items)\n","\n","    def __getitem__(self, idx):\n","        return self.items[idx]\n","\n","# Adapter for torch lightning\n","class VideoData(pl.LightningDataModule):\n","\n","    def __init__(self, root):\n","        super().__init__()\n","        self.train_root = osp.join(root, \"Train\")\n","        self.test_root = osp.join(root, \"Test\")\n","\n","    def _dataloader(self, files):\n","        dataset = ClipDataset(files)\n","        dataloader = data.DataLoader(\n","            dataset,\n","            batch_size=4,\n","            num_workers=16,\n","            pin_memory=True,\n","            sampler=SubsetRandomSampler(torch.randint(high=len(dataset), size=(int(0.1 * len(dataset)),)))\n","        )\n","        return dataloader\n","\n","    def train_dataloader(self):\n","        return self._dataloader(self.train_root)\n","\n","    def val_dataloader(self):\n","        return self._dataloader(self.test_root)\n","\n","    def test_dataloader(self):\n","        return self.val_dataloader()"]},{"cell_type":"markdown","source":["# Imported Code\n","This code is copied directly from https://github.com/wilson1yan/VideoGPT"],"metadata":{"id":"wuRAGMnugoJc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZJneOgylJiH"},"outputs":[],"source":["def download(id, fname, root=os.path.expanduser('~/.cache/videogpt')):\n","    os.makedirs(root, exist_ok=True)\n","    destination = os.path.join(root, fname)\n","\n","    if os.path.exists(destination):\n","        return destination\n","\n","    gdown.download(id=id, output=destination, quiet=False)\n","    return destination\n","\n","def load_vqvae(model_name, device=torch.device('cpu'), root=os.path.expanduser('~/.cache/videogpt')):\n","          filepath = download(_VQVAE[model_name], model_name, root=root)\n","          vqvae = VQVAE.load_from_checkpoint(filepath).to(device)\n","          vqvae.eval()\n","          return vqvae"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OiHbIOppMheC"},"outputs":[],"source":["class VideoGPT(pl.LightningModule):\n","    def __init__(self, args):\n","        super().__init__()\n","        self.args = args\n","\n","        # Load VQ-VAE and set all parameters to no grad\n","        if not os.path.exists(args.vqvae):\n","            self.vqvae = load_vqvae(args.vqvae)\n","        else:\n","            self.vqvae =  VQVAE.load_from_checkpoint(args.vqvae)\n","        for p in self.vqvae.parameters():\n","            p.requires_grad = False\n","        self.vqvae.codebook._need_init = False\n","        self.vqvae.eval()\n","\n","        # ResNet34 for frame conditioning\n","        self.use_frame_cond = args.n_cond_frames > 0\n","        if self.use_frame_cond:\n","            frame_cond_shape = (args.n_cond_frames,\n","                                args.resolution // 4,\n","                                args.resolution // 4,\n","                                60)\n","            self.resnet = resnet34(1, (1, 4, 4), resnet_dim=60)\n","            self.cond_pos_embd = AddBroadcastPosEmbed(\n","                shape=frame_cond_shape[:-1], embd_dim=frame_cond_shape[-1]\n","            )\n","        else:\n","            frame_cond_shape = None\n","\n","        # VideoGPT transformer\n","        # print(vars(self.vqvae))\n","        self.shape = (self.args.sequence_length, self.args.resolution // 4,\n","                       self.args.resolution // 4)\n","\n","        self.fc_in = nn.Linear(self.vqvae.embedding_dim, args.hidden_dim, bias=False)\n","        self.fc_in.weight.data.normal_(std=0.02)\n","\n","        self.attn_stack = AttentionStack(\n","            self.shape, args.hidden_dim, args.heads, args.layers, args.dropout,\n","            args.attn_type, args.attn_dropout, args.class_cond_dim, frame_cond_shape\n","        )\n","\n","        self.norm = LayerNorm(args.hidden_dim, args.class_cond_dim)\n","\n","        self.fc_out = nn.Linear(args.hidden_dim, self.vqvae.n_codes, bias=False)\n","        self.fc_out.weight.data.copy_(torch.zeros(self.vqvae.n_codes, args.hidden_dim))\n","\n","        # caches for faster decoding (if necessary)\n","        self.frame_cond_cache = None\n","\n","        self.save_hyperparameters()\n","\n","    def get_reconstruction(self, videos):\n","        return self.vqvae.decode(self.vqvae.encode(videos))\n","\n","    def sample(self, n, batch=None):\n","        device = self.fc_in.weight.device\n","\n","        cond = dict()\n","        if self.use_frame_cond or self.args.class_cond:\n","            assert batch is not None\n","            video = batch['video']\n","\n","            if self.args.class_cond:\n","                label = batch['label']\n","                cond['class_cond'] = F.one_hot(label, self.args.class_cond_dim).type_as(video)\n","            if self.use_frame_cond:\n","                cond['frame_cond'] = video[:, :, :self.args.n_cond_frames]\n","\n","        samples = torch.zeros((n,) + self.shape).long().to(device)\n","        idxs = list(itertools.product(*[range(s) for s in self.shape]))\n","\n","        with torch.no_grad():\n","            prev_idx = None\n","            for i, idx in enumerate(tqdm(idxs)):\n","                batch_idx_slice = (slice(None, None), *[slice(i, i + 1) for i in idx])\n","                batch_idx = (slice(None, None), *idx)\n","                embeddings = self.vqvae.codebook.dictionary_lookup(samples)\n","\n","                if prev_idx is None:\n","                    # set arbitrary input values for the first token\n","                    # does not matter what value since it will be shifted anyways\n","                    embeddings_slice = embeddings[batch_idx_slice]\n","                    samples_slice = samples[batch_idx_slice]\n","                else:\n","                    embeddings_slice = embeddings[prev_idx]\n","                    samples_slice = samples[prev_idx]\n","\n","                logits = self(embeddings_slice, samples_slice, cond,\n","                              decode_step=i, decode_idx=idx)[1]\n","                # squeeze all possible dim except batch dimension\n","                logits = logits.squeeze().unsqueeze(0) if logits.shape[0] == 1 else logits.squeeze()\n","                probs = F.softmax(logits, dim=-1)\n","                samples[batch_idx] = torch.multinomial(probs, 1).squeeze(-1)\n","\n","                prev_idx = batch_idx_slice\n","            samples = self.vqvae.decode(samples)\n","            samples = torch.clamp(samples, -0.5, 0.5) + 0.5\n","\n","        return samples # BCTHW in [0, 1]\n","\n","\n","    def forward(self, x, targets, cond, decode_step=None, decode_idx=None):\n","        if self.use_frame_cond:\n","            if decode_step is None:\n","                cond['frame_cond'] = self.cond_pos_embd(self.resnet(cond['frame_cond']))\n","            elif decode_step == 0:\n","                self.frame_cond_cache = self.cond_pos_embd(self.resnet(cond['frame_cond']))\n","                cond['frame_cond'] = self.frame_cond_cache\n","            else:\n","                cond['frame_cond'] = self.frame_cond_cache\n","\n","        h = self.fc_in(x)\n","        h = self.attn_stack(h, cond, decode_step, decode_idx)\n","        h = self.norm(h, cond)\n","        logits = self.fc_out(h)\n","        loss = F.cross_entropy(shift_dim(logits, -1, 1), targets)\n","\n","        return loss, logits\n","\n","    def training_step(self, batch, batch_idx):\n","        self.vqvae.eval()\n","        x = batch\n","        cond = dict()\n","        if self.args.class_cond:\n","            label = batch['label']\n","            cond['class_cond'] = F.one_hot(label, self.args.class_cond_dim).type_as(x)\n","        if self.use_frame_cond:\n","            cond['frame_cond'] = x[:, :, :self.args.n_cond_frames]\n","\n","        with torch.no_grad():\n","            targets, x = self.vqvae.encode(x, include_embeddings=True)\n","            x = shift_dim(x, 1, -1)\n","\n","        loss, _ = self(x, targets, cond)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        loss = self.training_step(batch, batch_idx)\n","        self.log('val/loss', loss, prog_bar=True)\n","        wandb.log({'val/loss': loss})\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=3e-4, betas=(0.9, 0.999))\n","        assert hasattr(self.args, 'max_steps') and self.args.max_steps is not None, f\"Must set max_steps argument\"\n","        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, self.args.max_steps)\n","        return [optimizer], [dict(scheduler=scheduler, interval='step', frequency=1)]\n","\n","\n","    @staticmethod\n","    def add_model_specific_args(parent_parser):\n","        parser = argparse.ArgumentParser(parents=[parent_parser], add_help=False)\n","        parser.add_argument('--vqvae', type=str, default='kinetics_stride4x4x4',\n","                            help='path to vqvae ckpt, or model name to download pretrained')\n","        parser.add_argument('--n_cond_frames', type=int, default=7)\n","        parser.add_argument('--class_cond', action='store_true')\n","\n","        # VideoGPT hyperparmeters\n","        parser.add_argument('--hidden_dim', type=int, default=576)\n","        parser.add_argument('--heads', type=int, default=4)\n","        parser.add_argument('--layers', type=int, default=8)\n","        parser.add_argument('--dropout', type=float, default=0.2)\n","        parser.add_argument('--attn_type', type=str, default='full',\n","                            choices=['full', 'sparse'])\n","        parser.add_argument('--attn_dropout', type=float, default=0.3)\n","\n","        return parser\n"]},{"cell_type":"markdown","source":["# Train Loop"],"metadata":{"id":"xStxz793gwv-"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"R894rUBEY7zO"},"outputs":[],"source":["import gc\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","wandb.login(key=\"6839b73984bcab4842ff37c1f24655fdb496167e\") #API Key is in your wandb account, under settings (wandb.ai/settings)\n","\n","class Args(argparse.Namespace):\n","    data_path = root+'Data'\n","    vqvae = root+'VideoGPT VQ-VAE/jpn9i0q0/checkpoints/epoch=44-step=29340.ckpt'\n","    n_cond_frames = 16\n","    resolution = 64\n","    sequence_length = 31\n","    batch_size = 1\n","    num_workers = 8\n","    hidden_dim = 288\n","    max_steps = 100\n","    max_epochs = 100\n","    heads = 4\n","    layers = 5 # 6\n","    dropout = 0.2\n","    attn_type = \"full\"\n","    attn_dropout = 0.3\n","    class_cond = None\n","    gpus = 1\n","\n","def main():\n","    pl.seed_everything(1234)\n","    args = Args()\n","\n","    data = VideoData(args.data_path)\n","    # pre-make relevant cached files if necessary\n","    data.train_dataloader()\n","    data.test_dataloader()\n","\n","    args.class_cond_dim = data.n_classes if args.class_cond else None\n","    model = VideoGPT(args)\n","\n","    callbacks = []\n","    callbacks.append(ModelCheckpoint(monitor='val/loss', mode='min', save_top_k=-1))\n","\n","    kwargs = dict()\n","    if args.gpus > 1:\n","        # find_unused_parameters = False to support gradient checkpointing\n","        kwargs = dict(distributed_backend='ddp', gpus=args.gpus,\n","                      plugins=[pl.plugins.DDPPlugin(find_unused_parameters=False)])\n","\n","    wandb_logger = WandbLogger(project=\"VideoGPT VQ-VAE\")\n","\n","    trainer = pl.Trainer(accelerator=\"gpu\", max_epochs=200, logger = wandb_logger)\n","\n","    trainer.fit(model, data)\n","\n","\n","if __name__ == '__main__':\n","    main()\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}